{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holographic Reduced Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has some basic tests and demonstrations of a pure Python 3 implementation of holographic reduced representations (HRRs) as outlined in [Plate 1995](http://ieeexplore.ieee.org/document/377968/). __The current implementation does not use numpy, so it's somewhat slow.__\n",
    "\n",
    "HRRs are a type of hetero-associative distributed representation that can efficiently encode nested compositional structure. Items are represented by n-tuples of floating-point elements, and encoded into pairs using circular convolution. The resulting n-tuples, called traces, may be composed together via element-wise addition to produce new traces. Importantly, HRR item \"atoms\" and HRR memory trace \"molecules\" are of the same dimension. This means traces may themselves function as items, allowing for the representation of the nested structure required by e.g. higher-order predicates. Other types of DRs represent memory traces and items in different spaces, or have exponentially increasing representation size with each composition. HRRs avoid this, at the cost of reduced representation fidelity and added limitations on the distribution from which element values are drawn.\n",
    "\n",
    "## why?\n",
    "HRRs can be found in the recent development of differentiable neural computers, a geometric analogue of quantum computation, and distributed representations in cognitive science.\n",
    "\n",
    "---\n",
    "\n",
    "The notebook sections here are, roughly, demos of:\n",
    " - basic HRR encoding, decoding, composition\n",
    " - examples of representing frames and sentences from Plate 1995.\n",
    "\n",
    "and (probably of limited interest):\n",
    " - Sanity checks based on computed values compared to analytical results in Plate 1995\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "1. [Basic encoding/decoding](#basic)\n",
    "1. [Demos of more complex representations](#Demos)\n",
    "    1. Sentences\n",
    "    1. Frames\n",
    "    1.\n",
    "1. [Some sanity checks](#san)\n",
    "    1. element distribution\n",
    "    1. convolution working\n",
    "    1. basic encoding and decoding\n",
    "    1. computed xi and eta distributions -vs- analytical values\n",
    "1. Capacity of HRRs\n",
    "    1. computations\n",
    "    1. compared to other DRs\n",
    "1. Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hrr\n",
    "#for simulations, plotting, etc.\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import math\n",
    "from functools import reduce\n",
    "from pandas import DataFrame\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"basic\"></a>\n",
    "## Basic encoding, composition, and decoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking of dot products of decoded traces with items in memory:\n",
      "_i: {'i': 1.00671, 'k': 0.02976, 'j': 0.01616, 'h': 0.01358}\n",
      "_h: {'h': 1.00671, 'k': 0.04558, 'i': -0.01444, 'j': -0.022}\n",
      "_j: {'j': 0.95465, 'i': -0.01791, 'h': -0.03365, 'k': -0.1172}\n",
      "_k: {'k': 0.95465, 'h': 0.02525, 'i': -0.00298, 'j': -0.07946}\n",
      "i_: {'i': 1.03659, 'j': 0.04141, 'h': 0.00145, 'k': -0.00389}\n",
      "h_: {'h': 1.03659, 'k': 0.02767, 'i': -0.02004, 'j': -0.02498}\n",
      "j_: {'j': 0.98452, 'i': 0.02767, 'h': -0.00389, 'k': -0.10906}\n",
      "k_: {'k': 0.98452, 'h': 0.04141, 'i': -0.02498, 'j': -0.10008}\n"
     ]
    }
   ],
   "source": [
    "#some dummy items to represent\n",
    "items = ['h', 'i', 'j', 'k']\n",
    "\n",
    "#associative memory matching each item to its HRR representation\n",
    "M = {i:hrr.HRR(n_dims = 2048) for i in items}\n",
    "\n",
    "#convolve two pairs of items, and compose the resulting traces\n",
    "trace1 = M['j'].encode(M['k'])\n",
    "trace2 = M['h'].encode(M['i'])\n",
    "composed = trace1 + trace2\n",
    "\n",
    "_i = trace2.decode(M['h'])\n",
    "_h = trace2.decode(M['i'])\n",
    "_k = trace1.decode(M['j'])\n",
    "_j = trace1.decode(M['k'])\n",
    "\n",
    "#decode each item's partner from the composed trace\n",
    "i_ = composed.decode(M['h'])\n",
    "h_ = composed.decode(M['i'])\n",
    "k_ = composed.decode(M['j'])\n",
    "j_ = composed.decode(M['k'])\n",
    "\n",
    "print('Ranking of dot products of decoded traces with items in memory:')\n",
    "print('_i:', hrr.getClosest(_i, M, 4))\n",
    "print('_h:', hrr.getClosest(_h, M, 4))\n",
    "print('_j:', hrr.getClosest(_j, M, 4))\n",
    "print('_k:', hrr.getClosest(_k, M, 4))\n",
    "print('i_:', hrr.getClosest(i_, M, 4))\n",
    "print('h_:', hrr.getClosest(h_, M, 4))\n",
    "print('j_:', hrr.getClosest(j_, M, 4))\n",
    "print('k_:', hrr.getClosest(k_, M, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Demos\"></a>\n",
    "# Demos -- representing more complex structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sent\"></a>\n",
    "## Representing sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_features = ['being', 'person', 'state', 'food', 'fish', 'bread']\n",
    "role_features = ['object','agent']\n",
    "frame_labels = ['cause', 'eat', 'see']\n",
    "base_features = object_features + role_features + frame_labels\n",
    "\n",
    "#some convenience lists -- not to be stored in memory \n",
    "_names = ['mark', 'john', 'paul', 'luke']\n",
    "_foods = ['fish', 'bread']\n",
    "_states = ['hunger', 'thirst']\n",
    "identifiers = ['id_' + i for i in  _names + _foods + _states + [r + '_' + f for r in role_features for f in frame_labels]]\n",
    "#base feature memory\n",
    "M = {i:hrr.HRR() for i in identifiers+object_features+frame_labels+role_features}\n",
    "\n",
    "# for k,v in M.items(): print(k, v)\n",
    "# print(type((M['being'] + M['person'] + M['id_' + 'mark'])))\n",
    "\n",
    "#memories for...\n",
    "#people, e.g. mark\n",
    "P = {p:(M['being'] + M['person'] + M['id_' + p])/math.sqrt(3) for p in _names} #/(math.sqrt(3))\n",
    "#food, e.g. the_fish\n",
    "F = {('the_' + f):(M['food'] + M[f] + M['id_' + f])/math.sqrt(3) for f in _foods}\n",
    "#states, e.g. hunger\n",
    "S = {s:(M['state'] + M['id_' + s])/math.sqrt(2) for s in _states}\n",
    "#roles, e.g. agt_cause\n",
    "R = {(r + '_' + f):(M[r]+M['id_' + r + '_' + f])/math.sqrt(2) for f in frame_labels for r in role_features}\n",
    "\n",
    "#combine all into token and role memory\n",
    "tokens = {**P, **F, **S, **R, **M}\n",
    "T = tokens #alias to save some typing\n",
    "               \n",
    "#building sentences using the above tokens. Divide by sqrt(elements) to normalize vector lengths to 1\n",
    "sentences = {\n",
    "    1: (T['eat'] + T['agent_eat'].encode(T['mark']) + T['object_eat'].encode(T['the_fish']))/math.sqrt(3),\n",
    "    3: (T['eat'] + T['agent_eat'].encode(T['john']))/math.sqrt(2),\n",
    "    4: (T['see'] + T['agent_see'].encode(T['john']) + T['object_see'].encode(T['mark']))/math.sqrt(3),\n",
    "    5: (T['see'] + T['agent_see'].encode(T['john']) + T['object_see'].encode(T['the_fish']))/math.sqrt(3),\n",
    "    6: (T['see'] + T['agent_see'].encode(T['the_fish']) + T['object_see'].encode(T['john']))/math.sqrt(3)\n",
    "}\n",
    "sentences[2] = (T['cause'] + T['agent_cause'].encode(T['hunger']) + T['object_cause'].encode(sentences[1]))/math.sqrt(3)\n",
    "\n",
    "T={**T, **sentences}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token similarities (dot product):\n",
      "\n",
      "         mark   john   paul   luke   fish  bread  hunger  thirst\n",
      "mark    1.010  0.000  0.000  0.000  0.000  0.000   0.000   0.000\n",
      "john    0.698  1.079  0.000  0.000  0.000  0.000   0.000   0.000\n",
      "paul    0.694  0.754  1.022  0.000  0.000  0.000   0.000   0.000\n",
      "luke    0.678  0.709  0.714  1.050  0.000  0.000   0.000   0.000\n",
      "fish    0.012  0.029  0.020  0.048  0.999  0.000   0.000   0.000\n",
      "bread  -0.073 -0.109 -0.065 -0.085  0.002  0.931   0.000   0.000\n",
      "hunger -0.062 -0.049 -0.025 -0.011  0.067  0.040   1.010   0.000\n",
      "thirst  0.002  0.003 -0.002  0.005  0.032 -0.002   0.458   0.875 \n",
      "\n",
      "Sentence similarities (dot product):\n",
      "\n",
      "       S1     S2     S3     S4     S5     S6\n",
      "S1  1.031  0.000  0.000  0.000  0.000  0.000\n",
      "S2  0.050  1.095  0.000  0.000  0.000  0.000\n",
      "S3  0.685  0.044  1.044  0.000  0.000  0.000\n",
      "S4  0.100  0.084  0.245  1.098  0.000  0.000\n",
      "S5  0.299 -0.023  0.251  0.714  1.121  0.000\n",
      "S6 -0.050 -0.029 -0.058  0.531  0.204  1.058\n"
     ]
    }
   ],
   "source": [
    "#similarities of tokens\n",
    "token_order = _names + _foods + _states\n",
    "token_sims = np.zeros((len(token_order), len(token_order)))\n",
    "for tr in range(len(token_order)):\n",
    "    for tc in range(tr+1):\n",
    "        token_sims[tr][tc] = round(T[token_order[tr]] * T[token_order[tc]], 3)\n",
    "        \n",
    "#similarities of sentences\n",
    "sentence_order = sorted(sentences.keys())\n",
    "sentence_sims = np.zeros((len(sentences), len(sentences)))\n",
    "for tr in range(len(sentence_order)):\n",
    "    for tc in range(tr+1):\n",
    "        sentence_sims[tr][tc] = round(sentences[sentence_order[tr]] * sentences[sentence_order[tc]], 3)\n",
    "\n",
    "\n",
    "print(\"Token similarities (dot product):\\n\")\n",
    "print(DataFrame(token_sims, token_order, token_order), '\\n')\n",
    "print(\"Sentence similarities (dot product):\\n\")\n",
    "print(DataFrame(sentence_sims, [\"S\"+str(i) for i in sentence_order], [\"S\"+str(i) for i in sentence_order]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting fillers and roles from frames\n",
    "\"[...]frame is convolved with the approximate inverse of the role and the result is cleaned up by choosing the most similar vector in the item memory.\"\n",
    "\n",
    "(recreating Table 8 from Plate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent of s1\n",
    "agent = sentences[1].decode(tokens['agent_eat'])\n",
    "results = hrr.getClosest(agent, T)\n",
    "print(\"agent of s1\")\n",
    "for k in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(k)\n",
    "#agent of s1(w/o frame label)\n",
    "agent = sentences[1].decode(tokens['agent'])\n",
    "results = hrr.getClosest(agent, T)\n",
    "print(\"agent of s1(w/o frame label)\")\n",
    "for k in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(k)\n",
    "# object of s1\n",
    "agent = sentences[1].decode(tokens['object_eat'])\n",
    "results = hrr.getClosest(agent, T)\n",
    "print(\"object of s1\")\n",
    "for k in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(k)\n",
    "# agent of s2\n",
    "agent = sentences[2].decode(tokens['agent_cause'])\n",
    "results = hrr.getClosest(agent, T)\n",
    "print(\"agent of s2\")\n",
    "for k in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(k)\n",
    "# object of s2\n",
    "agent = sentences[2].decode(tokens['object_cause'])\n",
    "results = hrr.getClosest(agent, T)\n",
    "print(\"object of s2\")\n",
    "for k in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(k)\n",
    "# agent of object of s2\n",
    "agent = sentences[2].decode(tokens['object_cause'].encode(tokens['agent_eat']))\n",
    "results = hrr.getClosest(agent, T)\n",
    "print(\"agent of object of s2\")\n",
    "for k in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(k)\n",
    "# object of object of s2\n",
    "agent = sentences[2].decode(tokens['object_cause'].encode(tokens['object_eat']))\n",
    "results = hrr.getClosest(agent, T)\n",
    "print(\"object of object of s2\")\n",
    "for k in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(k)\n",
    "# role of 'john' in s4\n",
    "agent = sentences[4].decode(tokens['john'])\n",
    "results = hrr.getClosest(agent, T)\n",
    "print(\"role of 'john' in s4\")\n",
    "for k in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(k)\n",
    "# role of 'john' in s5\n",
    "agent = sentences[5].decode(tokens['john'])\n",
    "results = hrr.getClosest(agent, T)\n",
    "print(\"role of 'john' in s5\")\n",
    "for k in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sequences\n",
    "seq = 'abc'\n",
    "\n",
    "#Stacks\n",
    "#Chunking sequences\n",
    "#Variable binding\n",
    "#Frames(slot/filler)\n",
    "#recursive frames (WHERE HRRs SHINE)\n",
    "    #commutativity limitation of HRRs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# addition memory capacity\n",
    "n=64 #elements in each addition memory vector\n",
    "k=3 #distinct items to be stored in the memory trace\n",
    "m=100 #total possible items \n",
    "samples = 100\n",
    "out_samples = np.empty((samples, m-k))\n",
    "in_samples = np.empty((samples, k))\n",
    "#finding optimal threshold for \n",
    "for s in range(samples):\n",
    "    M = {i:hrr.AdditionMemory(n_dims=64) for i in range(m)}\n",
    "    mems_in_trace = np.random.choice(range(m), 3, replace=False)\n",
    "    trace = reduce(lambda x,y: x+y, [M[i] for i in mems_in_trace]) #sum() does not work on trace here--why?\n",
    "    out_samples[s] = [M[o]*trace for o in range(m) if o not in mems_in_trace]\n",
    "    in_samples[s] = [M[i]*trace for i in mems_in_trace]\n",
    "outs = out_samples.reshape(samples*(m-k),1)\n",
    "ins = in_samples.reshape(samples*k,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poor man's optimization with probabilistic constraints\n",
    "#finding the threshold\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "\n",
    "def classifyDummyItems(t, n=64, m=100, k=3, mode='dist', iterations=100):\n",
    "    data = np.empty(100)\n",
    "    if mode=='dist': #with signal drawn from analytical distributions\n",
    "        for i in range(iterations):\n",
    "            in_data = np.random.normal(1, math.sqrt((k+1)/n), k) #dp of an item in memory\n",
    "            out_data = np.random.normal(0, math.sqrt(k/n), (m-k)) #dp of an item not in memory\n",
    "            data[i] = 1-((np.count_nonzero(in_data > t)/len(in_data))**k) * ((np.count_nonzero(out_data < t)/len(out_data))**(m-k)) #incorrects/total\n",
    "    elif mode=='items': #with signal computed from items\n",
    "        for i in iterations:\n",
    "            trace = sum([hrr.AdditionMemory(n_dims = n) for j in k])\n",
    "            \n",
    "    #score this t's performance\n",
    "    return np.mean(data)\n",
    "\n",
    "dist_min = minimize_scalar(lambda x: classifyDummyItems(x, n=64, m=100, k=3, mode='dist'), 0.5, bounds=(-2, 2), method='bounded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#end: plot s_a dist and histogram, s_r dist and histogram, threshold, \n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "x = np.linspace(-2, 2, 500)\n",
    "n_o, bins_o, patches_o = plt.hist(outs, bins='auto', density=True, alpha=0.5)\n",
    "n_i, bins_i, patches_i = plt.hist(ins, bins='auto', density=True, alpha=0.5)\n",
    "plt.plot(x,mlab.normpdf(x, 1, math.sqrt((k+1)/n)), label='accept') \n",
    "plt.plot(x,mlab.normpdf(x, 0, math.sqrt(k/n)), label='reject')\n",
    "plt.axis([-1.5, 2.5, 0, 2])\n",
    "plt.xlabel('dot product with trace', fontsize=24)\n",
    "plt.ylabel('probability', fontsize=24)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above took four items and gave them unique HRR \"signatures\" as representations. Two pairs of items' HRRs were convolved together into new HRRs, and these resulting traces were composed together. This final HRR represents two pairs of items.\n",
    "\n",
    "By taking any item's HRR, and decoding the composed trace, we can return an HRR representing the item's pair. Comparing the decoded HRR to those in an associative memory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few sanity checks\n",
    "<a id=\"san\" ></a>\n",
    "- verify elements of convolution\n",
    "- commutativity of convolution\n",
    "- xi and eta (noise terms) compared to theoretical values\n",
    "- decoding terms not in a trace should result in noise?\n",
    "- TODO: means and variances of commmon convolution ops (cf Table 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elements of our HRRs\n",
    "Q = 10\n",
    "dim = 512\n",
    "samples = np.empty((Q, dim))\n",
    "for q in range(Q):\n",
    "    samples[q] = hrr.HRR().values\n",
    "samples = samples.reshape((Q*dim,1))\n",
    "                \n",
    "plt.figure(figsize=(20, 10))\n",
    "sigma = math.sqrt(1/dim)\n",
    "x = np.linspace(-3*sigma, 3*sigma, 50)\n",
    "# n, bins, patches = plt.hist(samples, bins=50, density=True, range=(-3*sigma, 3*sigma))\n",
    "n, bins, patches = plt.hist(samples,bins='auto', density=True)\n",
    "plt.axis([-3*sigma, 3*sigma, 0, 15])\n",
    "plt.plot(x,mlab.normpdf(x, 0, sigma))\n",
    "plt.xlabel('element value', fontsize=24)\n",
    "plt.ylabel('probability', fontsize=24)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#is encoding working as intended?\n",
    "# C encoding X (c.encode(x))\n",
    "C = ['c0', 'c1', 'c2']\n",
    "X = ['x0', 'x1', 'x2']\n",
    "n = len(C)\n",
    "\n",
    "#circular convolution (HRR)\n",
    "#from hrr.py class HRR method convolve() (aka encode):\n",
    "hrr_terms = [' + '.join([C[k%n]+'*'+X[(j-k)%n] for k in range(0,len(X))]) for j in range(0, len(C))]\n",
    "\n",
    "#aperiodic convolution\n",
    "J = range(-(n-1), n)\n",
    "K = range(int(-(n-1)/2), int((n-1)/2)+1)\n",
    "aper_terms = []\n",
    "for j in J:\n",
    "    tmp_sum = []\n",
    "    for k in K:\n",
    "        # print('j:',j,'k:',k)\n",
    "        item_ind = k+int((n-1)/2)\n",
    "        self_ind = j-k+int((n-1)/2) \n",
    "        if self_ind >= 0 and self_ind < n and item_ind >= 0 and item_ind < n:\n",
    "            # print('ITEM:',item_ind)\n",
    "            # print('SELF:',self_ind)\n",
    "            tmp_sum.append(X[item_ind] + '*' + C[self_ind])\n",
    "    aper_terms.append(' + '.join(tmp_sum))\n",
    "    \n",
    "#truncated aperiodic convolution\n",
    "J = range(int(-(n-1)/2), int((n-1)/2)+1)\n",
    "K = range(int(-(n-1)/2), int((n-1)/2)+1)\n",
    "trunc_terms = []\n",
    "for j in J:\n",
    "    tmp_sum = []\n",
    "    for k in K:\n",
    "        item_ind = k+int((n-1)/2)\n",
    "        self_ind = j-k+int((n-1)/2)\n",
    "        if self_ind >= 0 and self_ind < n and item_ind >= 0 and item_ind < n:\n",
    "            tmp_sum.append(X[item_ind] + '*' + C[self_ind])\n",
    "    trunc_terms.append(' + '.join(tmp_sum))\n",
    "\n",
    "print(C, ' encoding ', X, ' : \\n')\n",
    "print('HRR:\\n','\\n '.join(hrr_terms), '\\n')\n",
    "print('Aperiodic:\\n', '\\n '.join(aper_terms), '\\n')\n",
    "print('Truncated aperiodic:\\n', '\\n '.join(trunc_terms), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#is decoding working as intended?\n",
    "t = hrr_terms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the HRR convolution commutative?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take two random HRRs h1 and h2, \n",
    "#convolve them--h1.encode(h2) and h2.encode(h1)--\n",
    "# and compare the resulting traces\n",
    "h1 = hrr.HRR()\n",
    "h2 = hrr.HRR()\n",
    "\n",
    "t1 = h1.encode(h2)\n",
    "t2 = h2.encode(h1)\n",
    "\n",
    "#since we're using floating point, compare values with some (very small) tolerance\n",
    "print(np.allclose(np.array(t1.values),np.array(t2.values)))\n",
    "# print(np.array(t1.values)-np.array(t2.values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do xi and eta follow their theoretical distributions? How do they vary wrt their assumed distributions as a function of HRR dimension? Plate says n=16 is sufficient for assuming normality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #finding eta and xi via least squares\n",
    "# for q in range(Q):\n",
    "#     for p in range(q, Q):\n",
    "#         #since convolution is commutative, only compare one set of pairs. And ignore self-to-self comparisons\n",
    "#         r_ = M[p].decode(M[p].encode(M[q])) # get a noisy version of our vector q\n",
    "#         r = np.array(r_.values) \n",
    "#         a_ = np.array([M[q].values])\n",
    "#         a = np.eye(N)\n",
    "#         b = np.array(M[q].values)-r #\"dependent\" values, b = r[i] - q[i] : the difference between expected and actual element values\n",
    "# #         print(b)\n",
    "# #         vals = np.linalg.(a,b)[0] #\n",
    "# #         xi[q][p] = vals[0]\n",
    "#         eta[q][p] = b    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do eta and xi change with increasing n?\n",
    "plt.figure(figsize=(25, 10))\n",
    "\n",
    "#HRR dimension\n",
    "Ns=[4, 8, 16, 32, 64]\n",
    "#Number of samples\n",
    "Q=10000\n",
    "#store a label for each HRR\n",
    "# M={q:hrr.HRR(n_dims=N) for q in range(Q)}\n",
    "#data arrays\n",
    "# xi = np.empty((Q,Q))\n",
    "# xi[:] = np.nan #to not skew stats later\n",
    "# eta = np.empty((Q,Q,N)) #N is i for eta_i (element i in noise vector)\n",
    "# eta[:] = np.nan\n",
    "\n",
    "for N in Ns:\n",
    "    xi = np.empty((Q,1))\n",
    "    eta = np.empty((Q,N))\n",
    "    eta_mean = np.empty((Q,1))\n",
    "    eta_one = np.empty((Q,1))\n",
    "    elements = np.empty((Q, N*2))\n",
    "    for q in range(Q):\n",
    "        h1 = hrr.HRR(n_dims=N)\n",
    "        h2 = hrr.HRR(n_dims=N)\n",
    "        h = h1.decode(h1.encode(h2))\n",
    "    #     xi[q] = np.sum(np.square(np.random.normal(0, math.sqrt(1/N), (N,1))))-1\n",
    "        xi[q] = sum(map(lambda x: x**2, h1.values))-1\n",
    "        eta[q] = np.array(h.values) - (np.array(h2.values)*(1+xi[q]))\n",
    "        eta_mean[q] = np.nanmean(eta[q])\n",
    "        eta_one[q] = eta[q][0]\n",
    "        elements[q] = h1.values + h2.values\n",
    "    \n",
    "    #xi plot\n",
    "    plt.subplot(2,len(Ns),(Ns.index(N)+1))\n",
    "    sigma_xi = math.sqrt(2/N)\n",
    "    x = np.linspace(-3*sigma_xi, 3*sigma_xi, 50)\n",
    "    n, bins, patches = plt.hist(xi, bins='auto', density=True, range=(-3*sigma_xi, 3*sigma_xi))\n",
    "    y_axis = max(n)\n",
    "    plt.axis([-3*sigma_xi, 3*sigma_xi, 0, y_axis])\n",
    "    plt.plot(x,mlab.normpdf(x, 0, sigma_xi), label='N(mu=0,sigma=sqrt(2/N)')\n",
    "    plt.legend()\n",
    "    plt.title('N = '+str(N))\n",
    "    plt.xlabel('xi', fontsize=24)\n",
    "    plt.ylabel('probability', fontsize=24)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "\n",
    "    #eta plot\n",
    "    plt.subplot(2,len(Ns),(Ns.index(N)+6))\n",
    "    sigma_eta = math.sqrt((N-1)/(N**2))\n",
    "    x = np.linspace(-3*sigma_eta, 3*sigma_eta, 50)\n",
    "    # vals = eta.reshape(eta.size, 1)\n",
    "    vals = eta_one\n",
    "    n, bins, patches = plt.hist(vals, bins='auto', density=True, range=(-3*sigma_eta, 3*sigma_eta))\n",
    "    y_axis = max(n)\n",
    "    plt.axis([-3*sigma_eta, 3*sigma_eta, 0, y_axis])\n",
    "    plt.plot(x,mlab.normpdf(x, 0, sigma_eta), label='N(mu=0,sigma=sqrt((N-1)/(N**2))')\n",
    "    plt.legend()\n",
    "    plt.title('N = '+str(N))\n",
    "    plt.xlabel('eta', fontsize=16)\n",
    "    plt.ylabel('probability', fontsize=16)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The distributions of dot products of some common convolution expressions \n",
    "## (Plate 1995, Table 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 10\n",
    "data = np.empty((samples, 10))\n",
    "for s in range(samples):\n",
    "    a,b,c,d = [hrr.HRR()]*4\n",
    "    data[s]=[a*a, a*b, a*(a.encode(b)), a*(b.encode(c)), (a.encode(a))*(a.encode(a)),\\\n",
    "             (a.encode(b))*(a.encode(b)), (a.encode(b))*(a.encode(a)), \\\n",
    "             (a.encode(b))*(a.encode(c)), (a.encode(b))*(c.encode(c)), \\\n",
    "             (a.encode(b))*(c.encode(d))]\n",
    "\n",
    "dists={\n",
    "    0:lambda x: (1, 2/x),\n",
    "    1:lambda x: (0, 1/x),\n",
    "    2:lambda x: (0, (((2*x)+1))/(x**2)),\n",
    "    3:lambda x: (0, 1/x),\n",
    "    4:lambda x: ((((2*x)+2))/(x), (((40*x)+112))/(x**2)),\n",
    "    5:lambda x: (1, (((6*x)+4))/(x**2)),\n",
    "    6:lambda x: (0, (((6*x)+18))/(x**2)),\n",
    "    7:lambda x: (0, (((2*x)+2))/(x**2)),\n",
    "    8:lambda x: (0, (((2*x)+2))/(x**2)),\n",
    "    9:lambda x: (0, 1/x),\n",
    "}\n",
    "\n",
    "# for i in range(1):\n",
    "#     plt.subplot(5,2,i)\n",
    "#     plt.hist(data[:][i], density=True)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# sigma_xi = math.sqrt(2/N)\n",
    "# x = np.linspace(-3*sigma_xi, 3*sigma_xi, 50)\n",
    "# n, bins, patches = plt.hist(xi, bins='auto', density=True, range=(-3*sigma_xi, 3*sigma_xi))\n",
    "# y_axis = max(n)\n",
    "# plt.axis([-3*sigma_xi, 3*sigma_xi, 0, y_axis])\n",
    "# plt.plot(x,mlab.normpdf(x, 0, sigma_xi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#HRR\n",
    "#evaluate percent correct retrievals as a function of encoded pairs\n",
    "\n",
    "#dimension (number of elements) of HRR to evaluate\n",
    "N = [8, 16, 32, 64, 128, 256]\n",
    "#number of pairs to encode in a single HRR\n",
    "P = [1]+list(range(5,26,5))\n",
    "#number of samples to take of each (dimension, NumPairs) pair\n",
    "samples = 25\n",
    "\n",
    "\n",
    "data = np.empty((len(N), len(P), samples))\n",
    "\n",
    "for n in range(len(N)):\n",
    "    for p in range(len(P)):\n",
    "        for s in range(samples):\n",
    "            X = {(str(i)+'x'):hrr.HRR(n_dims=N[n]) for i in range(P[p])}\n",
    "#             print('X', X)\n",
    "            Y = {(str(i)+'y'):hrr.HRR(n_dims=N[n]) for i in range(P[p])}\n",
    "#             print('Y', Y)\n",
    "            M = dict(list(X.items()) + list(Y.items()))\n",
    "#             print('M', M)\n",
    "            pairs = [X[str(i)+'x'].encode(Y[str(i)+'y']) for i in range(P[p])]\n",
    "            trace = reduce((lambda x,y: x+y), pairs)\n",
    "            score = 0\n",
    "            for h in range(P[p]):\n",
    "                ans = list(hrr.getClosest(item=trace.decode(M[str(h)+'x']), memoryDict=M, howMany=1))[0]\n",
    "                if ans == str(h)+'y':\n",
    "#                     print(str(h)+'x', '->', ans, '?=', str(h)+'y', '  +1')\n",
    "                    score+=1\n",
    "#                 else:\n",
    "#                     print(str(h)+'x', '->', ans, '?=', str(h)+'y', '  +0')\n",
    "                ans = list(hrr.getClosest(item=trace.decode(M[str(h)+'y']), memoryDict=M, howMany=1))[0]\n",
    "                if ans == str(h)+'x':\n",
    "#                     print(str(h)+'y', '->', ans, '?=', str(h)+'x', '  +1')\n",
    "                    score+=1\n",
    "#                 else:\n",
    "#                     print(str(h)+'y', '->', ans, '?=', str(h)+'x', '  +0')\n",
    "#             print(score/P[p])\n",
    "            data[n][p][s]= score / (P[p]*2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = plt.figure(figsize=(20, 10))\n",
    "x=list(P)\n",
    "\n",
    "means = {N[n]:np.mean(data[n],axis=1) for n in range(len(N))}\n",
    "SDs = {N[n]:np.std(data[n],axis=1) for n in range(len(N))}\n",
    "# print(SDs)\n",
    "\n",
    "for n in N:\n",
    "    plt.errorbar(x, means[n], SDs[n], label=str(n),) \n",
    "\n",
    "# plt.plot(np.linspace(start=1,stop=max(P),retstep=1))\n",
    "plt.axis([0, max(P)+1, 0, 1])\n",
    "plt.xlabel('Number of stored pairs', fontsize=24)\n",
    "plt.ylabel('Probability of correct decoding', fontsize=24)\n",
    "plt.xticks(x,fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "L=plt.legend(title='Number of vector elements', fontsize=18)\n",
    "plt.setp(L.get_title(),fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addition Memories\n",
    "Addition memories are perhaps the simplest version of a vector memory using float-valued elements. Vector representations are encoded by computing their element-wise sum, yielding a new vector referred to as a memory trace. Decoding (recognition), for our purposes, is done by taking the dot product of a vector with a memory trace and comparing it with a decision threshold dividing two dot product distributions: that of vectors in the trace, and that of vectors not in the trace.\n",
    "\n",
    "\n",
    "- $k$ distinct items stored out of $m$ possible items.\n",
    "- $n$ elements in each vector, each independently drawn from $ \\mathcal{N}(0, \\frac{1}{n})$.\n",
    "- $q$ probability of error in recognition.\n",
    "- $s_a$ and $s_r$ accept and reject signals.\n",
    "\n",
    "Decoding works by taking the dot product of a vector with the memory trace. The resulting value is modeled to have been drawn from one of two distributions: \"Accept\", corresponding to those vectors comprising the trace, and \"Reject\", for those vectors in our set of possible vectors but not stored in the trace.\n",
    "\n",
    "$$s_a \\stackrel{d}{=}  \\mathcal{N}(1, \\frac{k+1}{n})$$\n",
    "\n",
    "$$s_r \\stackrel{d}{=}  \\mathcal{N}(0, \\frac{k}{n})$$\n",
    "\n",
    "if $\\mathbf{x} \\cdot \\mathbf{t} $ is above a threshold $t$ then we assume it's in accept distribution, otherwise in reject threshold.\n",
    "\n",
    "$\\Pr(\\text{Hit}) = \\Pr(s_a > t)$ correctly recognizing a vector as stored in the trace.\n",
    "\n",
    "$\\Pr(\\text{Reject}) = \\Pr(s_{r} < t)$ correctly rejecting a vector as not stored in the trace.\n",
    "\n",
    "$$\\Pr(\\text{Correct}) = \\max_\\limits{t} \\Pr(\\text{Hit})^k \\Pr(\\text{Reject})^{m-k}$$\n",
    "\n",
    "\n",
    "determine threshold for given n, m, k\n",
    "\n",
    "numerical approximation:\n",
    "$$n = 3.16(k - 0.25)\\ln\\frac{m}{q^3}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
